# Data Lake ETL Pipeline
This projects implements an ETL pipeline for a data lake hosted on S3. The ETL pipelien loads data from S3, processes the data into analytics tables using PySpark, and loads them back into S3.  The data resides in S3, in a directory of JSON logs on user activity, as well as a directory with JSON metadata on the songs. The project defines fact and dimension tables for a star schema.

# Porject Structure
The spark folder contains the code for the ETL pipeline:
- **etl.py**: Contains the ETL pipeline written in Python and PySpark
- **dl.cfg**: Cotains the AWS credentials

# Data Set
The data consists of a song and a log data set for song play analysis.

### Song Data Set
The song dataset is a subset of real data from the Million Song Dataset. Each file is in JSON format and contains metadata about a song and the artist of that song. The files are partitioned by the first three letters of each song's track ID.
The song dataset resides in a S3 bucket and this is the link for it:
```s3://udacity-dend/song_data```
Here are filepaths to two files in this dataset:
```
song_data/A/B/C/TRABCEI128F424C983.json
song_data/A/A/B/TRAABJL12903CDCF1A.json
```
And below is an example of what a single song file looks like.
```
{"num_songs": 1, "artist_id": "ARJIE2Y1187B994AB7", "artist_latitude": null, "artist_longitude": null, "artist_location": "", "artist_name": "Line Renaud", "song_id": "SOUPIRU12A6D4FA1E1", "title": "Der Kleine Dompfaff", "duration": 152.92036, "year": 0}
```

### Log Data Set
The second dataset consists of log files in JSON format generated by this [event simulator](https://github.com/Interana/eventsim) based on the songs in the dataset above. These simulate activity logs from a music streaming app based on specified configurations.
The log data set resides in a S3 bucket and this is the link for it:
```s3://udacity-dend/log_data```
To properly load the log data into tables on the redshift a json path needs to be used:
```s3://udacity-dend/log_json_path.json```

The log files in the dataset you'll be working with are partitioned by year and month. For example, here are filepaths to two files in this dataset:
```
log_data/2018/11/2018-11-12-events.json
log_data/2018/11/2018-11-13-events.json
```
And below is an example of what the data in a log file, 2018-11-12-events.json, looks like.
![image_1](img/log_data_example.png)

# Tables
The ```etl.py``` script writes the following analytics tables to a S3 bucket as parquet files:

### Fact Tables
**songplays** - records in log data associated with song plays i.e. records with page ```NextSong```
songplay_id, start_time, user_id, level, song_id, artist_id, session_id, location, user_agent

### Dimension Tables
- **users** - users in the app
user_id, first_name, last_name, gender, level

- **songs** - songs in music database
song_id, title, artist_id, year, duration

- **artists** - artists in music database
artist_id, name, location, lattitude, longitude

- **time** - timestamps of records in songplays broken down into specific units
start_time, hour, day, week, month, year, weekday


# Run the Code

### Create a EMR Cluster
1. Create an AWS EMR Cluster:
   - Choose EMR versoin 5.32.0
   - As EMR-role use EMR_DefaultRole and add S3 FullAccess permissions to EMR_DefaultRole
2. Create a new AWS user with programmatic access and S3 Full Access permissions, save credentials in safe place and add the access_key and secret_key in the dl.cfg config file

### Copy the Python Code on the EMR Cluster:
```
scp -i <your permission file path> -r ./spark hadoop@<your Master DNS>:/home/hadoop/
```

### Connect to the EMR Cluster via SSH:
```
ssh -i <your permission file path> hadoop@<your Master DNS>
```
### Update .bashrc file
In the ssh session execute the following commands:
```
echo 'export SPARK_HOME=/usr/lib/spark' >> ~/.bashrc 
echo 'export PYTHONPATH=$SPARK_HOME/python/lib/py4j-X.XX.X-XXXX.zip:$PYTHONPATH' >> ~/.bashrc
echo 'export PYTHONPATH=$SPARK_HOME/python:$SPARK_HOME/python/build:$PYTHONPATH' >> ~/.bashrc
echo 'export PYSPARK_PYTHON=/usr/bin/python3' >> ~/.bashrc
source ~/.bashrc
```
Replace `XXX` with the actual version  found in the path `usr/lib/spark/python/lib` on the EMR cluster

### Run ETL Pipeline
In the ssh session navigate into the spark folder and run the `etl.py` script:
```
python etl.py
```

